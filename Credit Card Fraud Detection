import numpy as np
import pandas as pd
import scipy
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from pylab import rcParams
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
rcParams['figure.figsize'] = 14,8
RANDOM_SEED = 42
Labels = ['Normal', 'Fraud']
# Importing the data set
data = pd.read_csv('creditcard.csv')
print(data.head())
print(data.info())
print(data.shape)
print(data.columns)
# The below line shows the number of legitimate and fraud transactions:
# 1 indicates legitimate transactions and 0 indicates fraud transactions
print(data['Class'].value_counts())
print(data.describe())
# Checks if there are any null values in the data set
print(data.isnull().values.any())
count_classes = pd.value_counts(data['Class'], sort = True)
count_classes.plot(kind = 'bar', rot = 0)
plt.title('Transaction Class Distribution')
plt.xticks(range(2), Labels)
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.show()
# Get the fraud and normal data sets
normal = data[data['Class'] == 0]
fraud = data[data['Class'] == 1]
print(normal.shape)
print(fraud.shape)
# We need to analyse more information from the transaction data
# How different is the amount of money used in different transaction classes
print(fraud.Amount.describe())
print(normal.Amount.describe())
# Checking the amount per transaction by class
f, (ax1, ax2) = plt.subplots(2, 1, sharex = True)
f.suptitle('AMount by transaction per class')
ax1.hist(fraud.Amount, bins = 50)
ax1.set_title('Fraud')
ax2.hist(normal.Amount, bins = 50)
ax2.set_title('Normal')
plt.xlabel('Amount ($)')
plt.ylabel('Number of transactions')
plt.xlim(0, 20000)
plt.yscale('log')
plt.show();
# Checking how often does fraudulent transactions occur in time
f, (ax1, ax2) = plt.subplots(2, 1, sharex = True)
f.suptitle('Time of transactions vs Amount of class')
ax1.scatter(fraud.Time, fraud.Amount)
ax1.set_title('Fraud')
ax2.scatter(normal.Time, normal.Amount)
ax2.set_title('Normal')
plt.xlabel('Time in seconds')
plt.ylabel('Amount')
plt.show()
# Taking a part of the data set to make operations simple and understand the functioning better
data_part = data.sample(frac = 0.1, random_state = 1)
print(data_part.shape)
print(data.shape)
Fraud = data_part[data_part['Class'] == 1]
Normal = data_part[data_part['Class'] == 0]
outlier_fraction = len(Fraud) / float(len(Normal))
print(outlier_fraction)
print('Fraud cases : {}'.format(len(Fraud)))
print('Valid cases : {}'.format(len(Normal)))
# Correlation : getting correlations of each feature in the data set
corrmat = data_part.corr()
top_corr_features = corrmat.index
plt.figure(figsize = (20,20))
g = sns.heatmap(data[top_corr_features].corr(), annot = True, cmap = 'RdYlGn')
print(g)
# Create dependent and independent features
cols = list(data_part.columns)
# Filtering the columns which are not necessary
cols = [c for c in cols if c not in ['Class']]
# Storing the target variable for which we are making the predictions
target = 'Class'
state = np.random.RandomState(42)
X = data_part[cols]
Y = data_part[target]
X_outliers = state.uniform(low = 0, high = 1, size = (X.shape[0], X.shape[1]))
print(X.shape)
print(Y.shape)
classifiers = {'Isolation Forest': IsolationForest(n_estimators = 100, max_samples = len(X),
                contamination = outlier_fraction, random_state = state, verbose = 0),
               'Local Outlier Factor' : LocalOutlierFactor(n_neighbors = 20, algorithm = 'auto',
                leaf_size =30, metric = 'minkowski', p = 2, metric_params = None,
                contamination = outlier_fraction),
               'Support Vector Machine' : OneClassSVM(kernel = 'rbf', degree = 3, gamma = 0.1,
                nu = 0.05)}
print(type(classifiers))
n_outliers = len(Fraud)
for i, (clf_name, clf) in enumerate(classifiers.items()):
    # Fitting data and tag outliers
    if clf_name == 'Local Outlier Factor':
        y_pred = clf.fit_predict(X)
        scores_predict = clf.negative_outlier_factor_
    elif clf_name == 'Support Vector Machine':
        clf.fit(X)
        y_pred = clf.predict(X)
    else:
        clf.fit(X)
        scores_predict = clf.decision_function(X)
        y_pred = clf.predict(X)
    # Reshaping the prediction values: 0 for valid and 1 for fraud
    y_pred[y_pred == 1] = 0
    y_pred[y_pred == -1] = 1
    n_errors = (y_pred != Y).sum()
    # Run the classification metrics
    print(' {} {}'.format(clf_name, n_errors))
    print('Accuracy:', accuracy_score(Y, y_pred))
    print('Classification report:')
    print(classification_report(Y, y_pred))
